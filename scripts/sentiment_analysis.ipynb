{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with this script, we use VADER to conduct a sentiment analysis of all relevant books by Virginia Woolf. This will be done once with an unedited version of all sentences and once with an edited one - meaning with all words being lemmatized and stopwords being removed\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the first couple of operations are identital to what we already did in the word_count-script\n",
    "\n",
    "path_of_folder = \"C:\\\\Users\\\\Jakob\\\\Desktop\\\\Werkzeuge Hausarbeit\\\\Virginia_Woolf_Files\\\\Bearbeitete_Downloads\"\n",
    "file_iter = os.scandir(path_of_folder)\n",
    "\n",
    "all_texts = {}\n",
    "\n",
    "for f in file_iter:\n",
    "    myfile = open(path_of_folder + \"\\\\\" + f.name, 'r', encoding='utf-8')\n",
    "    f_name = f.name[5:-4]\n",
    "    text = \"\"\n",
    "    \n",
    "    for line in myfile.readlines():\n",
    "        if (line.lower().startswith(\"chapter\")) or (line.strip().isdigit()):\n",
    "            continue\n",
    "        else:\n",
    "            text += line\n",
    "            \n",
    "    text = text.replace('\\n', ' ')\n",
    "    all_texts[f_name] = text\n",
    "    myfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#just like in the word_count_script, we now tokenize all texts. Here, however, there is no word-tokenization, as we will use VADER for analyzing not single words, but sentences\n",
    "\n",
    "sentences_all_texts = {}\n",
    "\n",
    "for key in all_texts:\n",
    "    sent_list = nltk.sent_tokenize(all_texts[key])\n",
    "    sentences_all_texts[key]=sent_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we want to create an alternative dictionary with all sentences, where all words are lemmatized and stopwords are removed\n",
    "#for a better code structure, stopword-removal and lemmatization will be implemented in a separate function\n",
    "\n",
    "def stopword_removal_lemmatization(sentence):\n",
    "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "    #now we need word-tokens again\n",
    "    word_tokens = nltk.word_tokenize(sentence)\n",
    "    \n",
    "    filtered_sentence = []\n",
    "    \n",
    "    for w in word_tokens:\n",
    "        if w.lower() not in stop_words:\n",
    "            filtered_sentence.append(w)\n",
    "    \n",
    "    #now the tokens in filtered_sentence will be lemmatized. For this, they first need to be POS-tagged\n",
    "    #these tags then need to be transformed to be able to lemmatize them with the WordNetLemmatizer\n",
    "    tagged_tokens = nltk.pos_tag(filtered_sentence)\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    lemmata = []\n",
    "    for tok_tag in tagged_tokens:\n",
    "        (tok,tag) = tok_tag\n",
    "        pos = ''\n",
    "        if tag.startswith('JJ'):\n",
    "            pos = 'a'\n",
    "        elif tag.startswith('NN'):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('V'):\n",
    "            pos = 'v'\n",
    "        if pos:\n",
    "            lemmata.append(lemmatizer.lemmatize(tok,pos))\n",
    "        else:\n",
    "            lemmata.append(tok)\n",
    "    \n",
    "    edited_sentence = str(\" \".join(lemmata))\n",
    "    return edited_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the new dictionary and fill it by calling the stopword_removal_lemmatization-function for every sentence of every book. This takes some time\n",
    "\n",
    "no_stop_lemma_all_texts = {}\n",
    "\n",
    "for key in sentences_all_texts:\n",
    "    edited_sentences = []\n",
    "    for sentence in sentences_all_texts[key]:\n",
    "        edited_sentence = stopword_removal_lemmatization(sentence)\n",
    "        edited_sentences.append(edited_sentence)\n",
    "    no_stop_lemma_all_texts[key] = edited_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we can do the actual sentiment analysis with VADER\n",
    "#define a function for creating a sentiment-dictionary for a sentence with VADER\n",
    "def sentiment_scores(sentence):\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    sentiment_dict = analyzer.polarity_scores(sentence)\n",
    "    return sentiment_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iterate over every sentence of every book and create a sentiment-dictionary out of it by calling the sentiment_scores-function\n",
    "#extract different kinds of sentiment information out of the dictionary\n",
    "#for every book, calculate the arithmetic mean of different sentiment-values and save all of this in a tuple which is then returned\n",
    "\n",
    "def sentiment_analysis(some_dict):\n",
    "    \n",
    "    sentiment_all_texts = {}\n",
    "    \n",
    "    for key in some_dict:\n",
    "        sentence_count = 0\n",
    "        \n",
    "        sentence_neg_count = 0\n",
    "        sentence_neu_count = 0\n",
    "        sentence_pos_count = 0\n",
    "\n",
    "        neg_count = 0\n",
    "        neu_count = 0\n",
    "        pos_count = 0\n",
    "        compount_count = 0\n",
    "\n",
    "        for sentence in some_dict[key]:\n",
    "            sentiment_dict = sentiment_scores(sentence)\n",
    "            neg_count += sentiment_dict['neg']\n",
    "            neu_count += sentiment_dict['neu']\n",
    "            pos_count += sentiment_dict['pos']\n",
    "            compount_count += sentiment_dict['compound']\n",
    "\n",
    "            sentence_count += 1\n",
    "\n",
    "            if sentiment_dict['compound'] >= 0.05:\n",
    "                sentence_pos_count += 1\n",
    "            elif sentiment_dict['compound'] <= -0.05:\n",
    "                sentence_neg_count += 1\n",
    "            else:\n",
    "                sentence_neu_count += 1\n",
    "        \n",
    "        neg_sentences_perc = sentence_neg_count/(sentence_count/100)\n",
    "        neu_sentences_perc = sentence_neu_count/(sentence_count/100)\n",
    "        pos_sentences_perc = sentence_pos_count/(sentence_count/100)\n",
    "        \n",
    "        avg_neg = neg_count/sentence_count\n",
    "        avg_neu = neu_count/sentence_count\n",
    "        avg_pos = pos_count/sentence_count\n",
    "        avg_comp = compount_count/sentence_count\n",
    "        \n",
    "        sentiment_tuple = (neg_sentences_perc,neu_sentences_perc,pos_sentences_perc,avg_neg,avg_neu,avg_pos,avg_comp)\n",
    "        \n",
    "        sentiment_all_texts[key] = sentiment_tuple\n",
    "\n",
    "    \n",
    "    return sentiment_all_texts\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#call the just defined function for both sentence-dictionaries. This will take some time\n",
    "final_sentiment_dict_unedited = sentiment_analysis(sentences_all_texts)\n",
    "final_sentiment_dict_no_stop_lemma = sentiment_analysis(no_stop_lemma_all_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the resulting dictionaries in a list of tuples which makes it easier to create a DataFrame out of it\n",
    "final_sentiment_tuple_unedited = [(k,v1,v2,v3,v4,v5,v6,v7) for k,(v1,v2,v3,v4,v5,v6,v7) in final_sentiment_dict_unedited.items()]\n",
    "final_sentiment_tuple_no_stop_lemma = [(k,v1,v2,v3,v4,v5,v6,v7) for k,(v1,v2,v3,v4,v5,v6,v7) in final_sentiment_dict_no_stop_lemma.items()] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a DataFrame out of these lists of tuples\n",
    "df_unedited = pd.DataFrame(final_sentiment_tuple_unedited, columns = ['name','neg_sentences_perc','neu_sentences_perc','pos_sentences_perc','avg_neg','avg_neu','avg_pos','avg_comp'])\n",
    "df_no_stop_lemma = pd.DataFrame(final_sentiment_tuple_no_stop_lemma, columns = ['name','neg_sentences_perc','neu_sentences_perc','pos_sentences_perc','avg_neg','avg_neu','avg_pos','avg_comp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>neg_sentences_perc</th>\n",
       "      <th>neu_sentences_perc</th>\n",
       "      <th>pos_sentences_perc</th>\n",
       "      <th>avg_neg</th>\n",
       "      <th>avg_neu</th>\n",
       "      <th>avg_pos</th>\n",
       "      <th>avg_comp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Voyage Out</td>\n",
       "      <td>23.964455</td>\n",
       "      <td>40.318188</td>\n",
       "      <td>35.717357</td>\n",
       "      <td>0.056750</td>\n",
       "      <td>0.862865</td>\n",
       "      <td>0.080377</td>\n",
       "      <td>0.077669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Night and Day</td>\n",
       "      <td>26.112366</td>\n",
       "      <td>33.550272</td>\n",
       "      <td>40.337362</td>\n",
       "      <td>0.062205</td>\n",
       "      <td>0.850129</td>\n",
       "      <td>0.087663</td>\n",
       "      <td>0.094508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Monday or Tuesday</td>\n",
       "      <td>21.792619</td>\n",
       "      <td>47.275923</td>\n",
       "      <td>30.931459</td>\n",
       "      <td>0.069490</td>\n",
       "      <td>0.838007</td>\n",
       "      <td>0.092510</td>\n",
       "      <td>0.053961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jacob's Room</td>\n",
       "      <td>21.629543</td>\n",
       "      <td>50.293083</td>\n",
       "      <td>28.077374</td>\n",
       "      <td>0.058438</td>\n",
       "      <td>0.870877</td>\n",
       "      <td>0.070682</td>\n",
       "      <td>0.043306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mrs Dalloway</td>\n",
       "      <td>22.769064</td>\n",
       "      <td>45.078421</td>\n",
       "      <td>32.152515</td>\n",
       "      <td>0.071012</td>\n",
       "      <td>0.836510</td>\n",
       "      <td>0.092484</td>\n",
       "      <td>0.072555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>To the Lighthouse</td>\n",
       "      <td>24.189881</td>\n",
       "      <td>42.950540</td>\n",
       "      <td>32.859579</td>\n",
       "      <td>0.062536</td>\n",
       "      <td>0.855915</td>\n",
       "      <td>0.081550</td>\n",
       "      <td>0.062279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Orlando</td>\n",
       "      <td>25.871667</td>\n",
       "      <td>38.265456</td>\n",
       "      <td>35.862877</td>\n",
       "      <td>0.063321</td>\n",
       "      <td>0.853260</td>\n",
       "      <td>0.083415</td>\n",
       "      <td>0.071983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The Waves</td>\n",
       "      <td>24.701349</td>\n",
       "      <td>48.728324</td>\n",
       "      <td>26.570328</td>\n",
       "      <td>0.068340</td>\n",
       "      <td>0.865676</td>\n",
       "      <td>0.065984</td>\n",
       "      <td>0.016515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Flush</td>\n",
       "      <td>29.468822</td>\n",
       "      <td>47.066975</td>\n",
       "      <td>23.464203</td>\n",
       "      <td>0.076055</td>\n",
       "      <td>0.865644</td>\n",
       "      <td>0.058298</td>\n",
       "      <td>-0.008477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>The Years</td>\n",
       "      <td>15.456293</td>\n",
       "      <td>64.317409</td>\n",
       "      <td>20.226298</td>\n",
       "      <td>0.046482</td>\n",
       "      <td>0.892418</td>\n",
       "      <td>0.061101</td>\n",
       "      <td>0.034301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Between the Acts</td>\n",
       "      <td>14.162473</td>\n",
       "      <td>67.864845</td>\n",
       "      <td>17.972682</td>\n",
       "      <td>0.044493</td>\n",
       "      <td>0.903048</td>\n",
       "      <td>0.052457</td>\n",
       "      <td>0.031151</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 name  neg_sentences_perc  neu_sentences_perc  \\\n",
       "0      The Voyage Out           23.964455           40.318188   \n",
       "1       Night and Day           26.112366           33.550272   \n",
       "2   Monday or Tuesday           21.792619           47.275923   \n",
       "3        Jacob's Room           21.629543           50.293083   \n",
       "4        Mrs Dalloway           22.769064           45.078421   \n",
       "5   To the Lighthouse           24.189881           42.950540   \n",
       "6             Orlando           25.871667           38.265456   \n",
       "7           The Waves           24.701349           48.728324   \n",
       "8               Flush           29.468822           47.066975   \n",
       "9           The Years           15.456293           64.317409   \n",
       "10   Between the Acts           14.162473           67.864845   \n",
       "\n",
       "    pos_sentences_perc   avg_neg   avg_neu   avg_pos  avg_comp  \n",
       "0            35.717357  0.056750  0.862865  0.080377  0.077669  \n",
       "1            40.337362  0.062205  0.850129  0.087663  0.094508  \n",
       "2            30.931459  0.069490  0.838007  0.092510  0.053961  \n",
       "3            28.077374  0.058438  0.870877  0.070682  0.043306  \n",
       "4            32.152515  0.071012  0.836510  0.092484  0.072555  \n",
       "5            32.859579  0.062536  0.855915  0.081550  0.062279  \n",
       "6            35.862877  0.063321  0.853260  0.083415  0.071983  \n",
       "7            26.570328  0.068340  0.865676  0.065984  0.016515  \n",
       "8            23.464203  0.076055  0.865644  0.058298 -0.008477  \n",
       "9            20.226298  0.046482  0.892418  0.061101  0.034301  \n",
       "10           17.972682  0.044493  0.903048  0.052457  0.031151  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check if DataFrame-setup worked\n",
    "df_unedited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>neg_sentences_perc</th>\n",
       "      <th>neu_sentences_perc</th>\n",
       "      <th>pos_sentences_perc</th>\n",
       "      <th>avg_neg</th>\n",
       "      <th>avg_neu</th>\n",
       "      <th>avg_pos</th>\n",
       "      <th>avg_comp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Voyage Out</td>\n",
       "      <td>22.502508</td>\n",
       "      <td>36.491329</td>\n",
       "      <td>41.006163</td>\n",
       "      <td>0.073472</td>\n",
       "      <td>0.803337</td>\n",
       "      <td>0.123190</td>\n",
       "      <td>0.112337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Night and Day</td>\n",
       "      <td>23.748174</td>\n",
       "      <td>31.451720</td>\n",
       "      <td>44.800106</td>\n",
       "      <td>0.078337</td>\n",
       "      <td>0.787602</td>\n",
       "      <td>0.134059</td>\n",
       "      <td>0.132330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Monday or Tuesday</td>\n",
       "      <td>20.562390</td>\n",
       "      <td>45.694200</td>\n",
       "      <td>33.743409</td>\n",
       "      <td>0.073873</td>\n",
       "      <td>0.814946</td>\n",
       "      <td>0.111181</td>\n",
       "      <td>0.077130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jacob's Room</td>\n",
       "      <td>20.867526</td>\n",
       "      <td>49.560375</td>\n",
       "      <td>29.572098</td>\n",
       "      <td>0.067657</td>\n",
       "      <td>0.843924</td>\n",
       "      <td>0.088424</td>\n",
       "      <td>0.055916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mrs Dalloway</td>\n",
       "      <td>20.876149</td>\n",
       "      <td>44.835046</td>\n",
       "      <td>34.288805</td>\n",
       "      <td>0.082544</td>\n",
       "      <td>0.797050</td>\n",
       "      <td>0.120402</td>\n",
       "      <td>0.091306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>To the Lighthouse</td>\n",
       "      <td>21.461057</td>\n",
       "      <td>42.382035</td>\n",
       "      <td>36.156907</td>\n",
       "      <td>0.078344</td>\n",
       "      <td>0.803622</td>\n",
       "      <td>0.118030</td>\n",
       "      <td>0.088968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Orlando</td>\n",
       "      <td>23.469089</td>\n",
       "      <td>37.943159</td>\n",
       "      <td>38.587753</td>\n",
       "      <td>0.079297</td>\n",
       "      <td>0.801791</td>\n",
       "      <td>0.118919</td>\n",
       "      <td>0.100176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The Waves</td>\n",
       "      <td>22.273603</td>\n",
       "      <td>49.691715</td>\n",
       "      <td>28.034682</td>\n",
       "      <td>0.079472</td>\n",
       "      <td>0.826746</td>\n",
       "      <td>0.093781</td>\n",
       "      <td>0.036466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Flush</td>\n",
       "      <td>27.852194</td>\n",
       "      <td>47.713626</td>\n",
       "      <td>24.434180</td>\n",
       "      <td>0.093440</td>\n",
       "      <td>0.824173</td>\n",
       "      <td>0.082384</td>\n",
       "      <td>0.007876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>The Years</td>\n",
       "      <td>14.701967</td>\n",
       "      <td>63.296850</td>\n",
       "      <td>22.001183</td>\n",
       "      <td>0.053322</td>\n",
       "      <td>0.869668</td>\n",
       "      <td>0.077009</td>\n",
       "      <td>0.044137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Between the Acts</td>\n",
       "      <td>13.425593</td>\n",
       "      <td>67.433501</td>\n",
       "      <td>19.140906</td>\n",
       "      <td>0.047874</td>\n",
       "      <td>0.886191</td>\n",
       "      <td>0.065934</td>\n",
       "      <td>0.039222</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 name  neg_sentences_perc  neu_sentences_perc  \\\n",
       "0      The Voyage Out           22.502508           36.491329   \n",
       "1       Night and Day           23.748174           31.451720   \n",
       "2   Monday or Tuesday           20.562390           45.694200   \n",
       "3        Jacob's Room           20.867526           49.560375   \n",
       "4        Mrs Dalloway           20.876149           44.835046   \n",
       "5   To the Lighthouse           21.461057           42.382035   \n",
       "6             Orlando           23.469089           37.943159   \n",
       "7           The Waves           22.273603           49.691715   \n",
       "8               Flush           27.852194           47.713626   \n",
       "9           The Years           14.701967           63.296850   \n",
       "10   Between the Acts           13.425593           67.433501   \n",
       "\n",
       "    pos_sentences_perc   avg_neg   avg_neu   avg_pos  avg_comp  \n",
       "0            41.006163  0.073472  0.803337  0.123190  0.112337  \n",
       "1            44.800106  0.078337  0.787602  0.134059  0.132330  \n",
       "2            33.743409  0.073873  0.814946  0.111181  0.077130  \n",
       "3            29.572098  0.067657  0.843924  0.088424  0.055916  \n",
       "4            34.288805  0.082544  0.797050  0.120402  0.091306  \n",
       "5            36.156907  0.078344  0.803622  0.118030  0.088968  \n",
       "6            38.587753  0.079297  0.801791  0.118919  0.100176  \n",
       "7            28.034682  0.079472  0.826746  0.093781  0.036466  \n",
       "8            24.434180  0.093440  0.824173  0.082384  0.007876  \n",
       "9            22.001183  0.053322  0.869668  0.077009  0.044137  \n",
       "10           19.140906  0.047874  0.886191  0.065934  0.039222  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_no_stop_lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export each DataFrame to a csv-file to be able to work with them later\n",
    "df_unedited.to_csv(r'C:\\\\Users\\\\Jakob\\\\Desktop\\\\Werkzeuge Hausarbeit\\\\Ergebnisse\\\\sentiment_analysis_unedited.csv', index=False)\n",
    "df_no_stop_lemma.to_csv(r'C:\\\\Users\\\\Jakob\\\\Desktop\\\\Werkzeuge Hausarbeit\\\\Ergebnisse\\\\sentiment_analysis_no_stop_lemma.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
